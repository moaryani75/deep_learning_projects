{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepration\n",
    "(xtr ,ytr) ,(xte ,yte) =mnist.load_data()\n",
    "xtr_p =(xtr /255).reshape(-1,784).astype(np.float32)\n",
    "xte_p =(xte /255).reshape(-1,784).astype(np.float32)\n",
    "ytr_p =ytr.reshape(-1,1)\n",
    "yte_p =yte.reshape(-1,1)\n",
    "plt.figure(num =1 ,figsize =(3,3) ,dpi =100)\n",
    "for i in range(len(xtr[:10])) :\n",
    "  plt.subplot(2 ,5 ,i+1)\n",
    "  plt.imshow(xtr[i] ,cmap ='gray')\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator network\n",
    "def create_generator() :\n",
    "  gen =keras.Sequential()\n",
    "  gen.add(keras.layers.Conv2D(filters =3 ,kernel_size =(2,2) ,input_shape =(28,28,1)))\n",
    "  gen.add(keras.layers.BatchNormalization())\n",
    "  gen.add(keras.layers.ReLU(max_value =1.0))\n",
    "  gen.add(keras.layers.AvgPool2D(pool_size=(2,2) ,strides=(1,1)))\n",
    "  gen.add(keras.layers.Conv2D(filters =6 ,kernel_size =(3,3)))\n",
    "  gen.add(keras.layers.BatchNormalization())\n",
    "  gen.add(keras.layers.ReLU(max_value =1.0))\n",
    "  gen.add(keras.layers.MaxPool2D(pool_size=(2,2) ,strides=(1,1)))\n",
    "  gen.add(keras.layers.Conv2D(filters =9 ,kernel_size =(4,4)))\n",
    "  gen.add(keras.layers.BatchNormalization())\n",
    "  gen.add(keras.layers.ReLU(max_value =1.0))\n",
    "  gen.add(keras.layers.AvgPool2D(pool_size=(2,2) ,strides=(1,1)))\n",
    "  gen.add(keras.layers.Conv2D(filters =12 ,kernel_size =(5,5)))\n",
    "  gen.add(keras.layers.BatchNormalization())\n",
    "  gen.add(keras.layers.ReLU(max_value =1.0))\n",
    "  gen.add(keras.layers.MaxPool2D(pool_size=(2,2) ,strides=(1,1)))\n",
    "  gen.add(keras.layers.Conv2D(filters =14 ,kernel_size =(7,6)))\n",
    "  gen.add(keras.layers.BatchNormalization())\n",
    "  gen.add(keras.layers.ReLU(max_value =1.0))\n",
    "  gen.add(keras.layers.AvgPool2D(pool_size=(2,2) ,strides=(1,1)))\n",
    "  gen.add(keras.layers.Flatten())\n",
    "  gen.compile(optimizer =keras.optimizers.Adam(learning_rate=0.0002) ,loss =keras.losses.BinaryCrossentropy)\n",
    "  return gen\n",
    "generator =create_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator network\n",
    "def create_discriminator() :\n",
    "  dis =keras.Sequential()\n",
    "  dis.add(keras.Input(shape =(784,)))\n",
    "  dis.add(keras.layers.Dense(units =512))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =256))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =128))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =64))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =32))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =16))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =8))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.ReLU(max_value =1.0))\n",
    "  dis.add(keras.layers.Dense(units =1))\n",
    "  dis.add(keras.layers.BatchNormalization())\n",
    "  dis.add(keras.layers.Activation(keras.activations.sigmoid))\n",
    "  dis.compile(optimizer =keras.optimizers.Adam(learning_rate =0.0002) ,loss =keras.losses.BinaryCrossentropy)\n",
    "  return dis\n",
    "discriminator =create_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan network\n",
    "def create_gan(generator ,discriminator) :\n",
    "  gan_input =keras.Input(shape =(28,28,1))\n",
    "  gen_output =generator(gan_input)\n",
    "  gan_output =discriminator(gen_output)\n",
    "  discriminator.trainable =False\n",
    "  gan =keras.Model(inputs =gan_input ,outputs =gan_output)\n",
    "  gan.compile(optimizer =keras.optimizers.Adam(learning_rate =0.0005) ,loss =keras.losses.BinaryCrossentropy)\n",
    "  return gan\n",
    "gan =create_gan(generator ,discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_plot function\n",
    "def save_plot(EPOCH ,EXM ,generator) : \n",
    "    noise_tensor =np.random.normal(loc =0.0 ,scale =1.0 ,size =[EXM ,28,28,1])\n",
    "    generated_img =generator.predict(noise_tensor).reshape(EXM ,28,28,1)\n",
    "    plt.figure(num =2 ,figsize =(5,5) ,dpi =100)\n",
    "    for i in range(generated_img.shape[0]) : \n",
    "      plt.subplot(10 ,10 ,i+1) \n",
    "      plt.imshow(generated_img[i] ,interpolation='nearest' ,cmap ='gray')\n",
    "      plt.axis('off') \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_img_%d.png' %EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "BATCH =60\n",
    "EPOCH =100\n",
    "BATCH_COUNT =int(xtr.shape[0] /BATCH)\n",
    "for e in range(1 ,EPOCH +1) :\n",
    "  for b in range(BATCH_COUNT) :\n",
    "    noise_tensor =np.random.normal(loc =0 ,scale =1 ,size =[BATCH ,28,28,1])\n",
    "    xtr_fake_batch =generator.predict(noise_tensor)\n",
    "    xtr_real_batch =xtr_p[np.random.randint(low =0 ,high =xtr.shape[0] ,size =BATCH)]\n",
    "    Xtr =np.concatenate((xtr_real_batch ,xtr_fake_batch))\n",
    "    y_dis =np.zeros((2*BATCH ,1))\n",
    "    y_dis[:BATCH,0] =1.0\n",
    "    discriminator.trainable =True\n",
    "    discriminator.train_on_batch(Xtr ,y_dis)\n",
    "    discriminator.trainable =False\n",
    "    y_gan =np.ones((BATCH ,1))\n",
    "    gan.train_on_batch(noise_tensor ,y_gan)\n",
    "    # model evaluation\n",
    "    EXM =100\n",
    "    save_plot(e ,EXM ,generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
